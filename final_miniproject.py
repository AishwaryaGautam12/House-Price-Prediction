# -*- coding: utf-8 -*-
"""final_miniproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nE_bn4d4AX3QtK-SjOrL-XTLhZLz1xkm

** Predicting House Prices using Housing Data of Bangalore city **
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing necessary Libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)

#Reading file to Pandas Dataframe
df1 = pd.read_csv("/content/Bengaluru_House_Data.csv")
df1.head()

df1.shape

"""1) Data Cleaning"""

df1.groupby('area_type')['area_type'].agg('count')

"""Here we are dropping a few columns to make model building easier."""

df2 = df1.drop(['area_type','society','balcony','availability'],axis = 'columns')

df2.head()

df2.isnull().sum()

df2['bath'].fillna(1)

df3 = df2.dropna()
df3.isnull().sum()

#Finding unique values in size column
df3['size'].unique()

df3['bhk']  = df3['size'].apply(lambda x: int(x.split(' ')[0]))
df3.head()

df3['bhk'].unique()

df3.total_sqft.unique()

"""Some values in total_sqft column are in the form of range. So we'll extract those columns and take average."""

#This fuction will check whether a particular value is float or not.
def isfloat(x):
  try:
    float(x)
  except:
    return False
  return True

df3[~df3['total_sqft'].apply(isfloat)].head(10)

def convert_sqft_to_num(x):
  tokens = x.split('-')
  if len(tokens) == 2:
    return (float(tokens[0]) + float(tokens[1]))/2
  try:
    return float(x)
  except:
    return None

df4 = df3.copy()
df4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)

df4.loc[30]

df4.isnull().sum()

df5 = df4.dropna()

df5.isnull().sum()

"""Now we are done with data cleaning.

2) Feature Engineering

First we will create a new column which will help us in dimensionality reduction as well as outlier removal.

In the real estate market, price per square feet is very important. So we'll create a new column called price_per_sq_ft.
"""

df5['price_per_sq_ft'] = df5['price']*100000/df5['total_sqft']
df5.head()

"""Now we will check how many unique locations are there."""

df5.location.unique()

len(df5.location.unique())

"""##### We cannot use One Hot Encoding directly due to Dimensionality Curse.

So any location which has number of houses < 10, will be labelled as 'other'
"""

# Removing any extra Space.
df5.location = df5.location.apply(lambda x: x.strip())
df5.head()

location_stats = df5.groupby('location')['location'].agg('count').sort_values(ascending = False)
location_stats

#Here location stats is a series. Therefore you can directly apply len function.
len(location_stats[location_stats<=10])

len(df5.location.unique())

locations_less_than_10 = location_stats[location_stats<=10]
locations_less_than_10

df5.location = df5.location.apply(lambda x: 'other' if x in locations_less_than_10 else x)
len(df5.location.unique())

df5.head(10)

"""3) Outlier Detection & Removal

According to research, typical square feet per bedroom is around 300. Area less than that will be an outlier.
"""

df5[df5.total_sqft/df5.bhk<300].head()

df5.shape

# Removing above values
df6 = df5[~(df5.total_sqft/df5.bhk<300)]
df6.shape

df6.price_per_sq_ft.describe()

"""The above cell shows that minimum value of price_per_sq_ft is extremely low and max value is extremely high. Thus there are high chances that above values might be outliers.
Thus we will remove them.

We use the concept of mean & standard deviation for outlier removal.
The standard deviation is a summary measure of the differences of each observation from the mean.
"""

def remove_outliers(df):
  df_out = pd.DataFrame()
  for key, subdf in df.groupby('location'):
    m = np.mean(subdf.price_per_sq_ft)
    sd = np.std(subdf.price_per_sq_ft)
    reduced_df = subdf[(subdf.price_per_sq_ft>(m-sd)) & (subdf.price_per_sq_ft<=(m+sd))]
    df_out = pd.concat([df_out,reduced_df],ignore_index = True)
  return df_out

df7 = remove_outliers(df6)
df7.shape

"""Now we will look at prices of 2bhk and 3bk apartments per total_sqft for any given loction."""

def plot_scatter_chart(df,location):
  bhk2 = df[((df.location == location) & (df.bhk == 2))]
  bhk3 = df[((df.location == location) & (df.bhk == 3))]
  #change the default plot size using matplotlib.rcParams
  matplotlib.rcParams["figure.figsize"] = (15,10)
  plt.scatter(bhk2.total_sqft,bhk2.price,marker = '*',color = 'blue', label = '2 bhk', s = 50)
  plt.scatter(bhk3.total_sqft,bhk3.price,marker = '+',color = 'green', label = '3 bhk', s = 50)
  plt.xlabel("Total area in square feet")
  plt.ylabel("Price ")
  plt.title(location)
  plt.legend()

plot_scatter_chart(df7,"Hebbal")

plot_scatter_chart(df7,"Rajaji Nagar")



"""##### We need to remove those properties where price of 2 bedroom apartments is more than price of 3 bedroom apartment(of same area) for the same location."""

def remove_bhk_outliers(df):
    exclude_indices = np.array([])
    for location, location_df in df.groupby('location'):
        bhk_stats = {}
        for bhk, bhk_df in location_df.groupby('bhk'):
            bhk_stats[bhk] = {
                'mean': np.mean(bhk_df.price_per_sq_ft),
                'std': np.std(bhk_df.price_per_sq_ft),
                'count': bhk_df.shape[0]
            }
        for bhk, bhk_df in location_df.groupby('bhk'):
            stats = bhk_stats.get(bhk-1)
            if stats and stats['count']>5:
                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sq_ft<(stats['mean'])].index.values)
    return df.drop(exclude_indices)

df8 = remove_bhk_outliers(df7)
df8.shape

plot_scatter_chart(df8,"Hebbal")

plot_scatter_chart(df8,"Rajaji Nagar")

import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)
plt.hist(df8.price_per_sq_ft,rwidth=0.8)
plt.xlabel("Price Per Square Feet")
plt.ylabel("Count")

df8.bath.unique()

"""##### It is unusual to have number of bathrooms = number of bedrooms + 2.
Therefore any property violating this must be an outlier.
"""

df8[df8.bath>df8.bhk+2]

df9 = df8[df8.bath<df8.bhk+2]
df9.shape

"""Now we can drop columns which are repetitive; like size and bhk provide the same information.

Similarly we can drop price_per_sq_ft as we have total_sqft available.
"""

df10 = df9.drop(['size','price_per_sq_ft'],axis='columns')
df10.head(3)

"""Location is Categorical. Therefore we perform One Hot Encoding."""

dummies = pd.get_dummies(df10.location)
dummies.head(3)

df11 = pd.concat([df10,dummies.drop('other',axis='columns')],axis='columns')
df11.head()

df12 = df11.drop('location',axis='columns')
df12.head(2)

df12.shape

"""This concludes pre-processing. 

Now we move on to building the actual model building.

#Building the Model

First we split our columns as dependent (price) and independent (area,number of berooms and bathrooms) variables.
"""

X = df12.drop(['price'],axis='columns')
X.head(3)

X.shape

y = df12.price
y.head(3)

"""We need to split our data into training and testing sets."""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)

#len(X_train['total_sqft'])
len(y_train)

"""Using Linear Regression"""

from sklearn.linear_model import LinearRegression
lr_clf = LinearRegression()
lr_clf.fit(X_train,y_train)
lr_clf.score(X_test,y_test)

"""Use K Fold cross validation to measure accuracy of our LinearRegression model"""

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score

cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

cross_val_score(LinearRegression(), X, y, cv=cv)

"""Thus, our model seems to be pretty acurate."""

from sklearn.model_selection import GridSearchCV

from sklearn.tree import DecisionTreeRegressor

def find_best_model_using_gridsearchcv(X,y):
    algos = {
        'linear_regression' : {
            'model': LinearRegression(),
            'params': {
                'normalize': [True, False]
            }
        },
        
        'decision_tree': {
            'model': DecisionTreeRegressor(),
            'params': {
                'criterion' : ['mse','friedman_mse'],
                'splitter': ['best','random']
            }
        }
    }
    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
    for algo_name, config in algos.items():
        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        gs.fit(X,y)
        scores.append({
            'model': algo_name,
            'best_score': gs.best_score_,
            'best_params': gs.best_params_
        })

    return pd.DataFrame(scores,columns=['model','best_score','best_params'])

find_best_model_using_gridsearchcv(X,y)

find_best_model_using_gridsearchcv(X,y)

"""Based on above results we can say that LinearRegression gives the best score. Hence we will use that."""

def predict_price(location,sqft,bath,bhk):    
    loc_index = np.where(X.columns==location)[0][0]

    x = np.zeros(len(X.columns))
    x[0] = sqft
    x[1] = bath
    x[2] = bhk
    if loc_index >= 0:
        x[loc_index] = 1

    return lr_clf.predict([x])[0]

predict_price('1st Phase JP Nagar',1000, 2, 2)

predict_price('1st Phase JP Nagar',1000, 3, 3)

predict_price('Indira Nagar',1000, 2, 2)

predict_price('Indira Nagar',1000, 3, 3)

